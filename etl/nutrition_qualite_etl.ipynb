{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL PySpark - OpenFoodFacts Datamart\n",
    "## Architecture Dimensionnelle (Star Schema)\n",
    "\n",
    "Ce notebook ex√©cute un pipeline ETL complet :\n",
    "- **Bronze** ‚Üí Lecture CSV brute\n",
    "- **Silver** ‚Üí Nettoyage, validation, d√©duplication\n",
    "- **Gold** ‚Üí Dimensions (SCD2) + Table de faits\n",
    "\n",
    "**Source** : 300k produits OpenFoodFacts\n",
    "**Destination** : PostgreSQL datamart `off_dm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0 : Setup et imports\n",
    "Configure le chemin du driver PostgreSQL et initialise les biblioth√®ques Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "# Configuration du driver PostgreSQL pour Spark\n",
    "# √Ä adapter selon votre chemin r√©el du JAR PostgreSQL\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    r'--jars \"C:\\Users\\Test\\Desktop\\TP Benoit\\postgresql-42.7.8.jar\" pyspark-shell'\n",
    ")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Constantes m√©tier\n",
    "\n",
    "D√©finit les r√®gles de validation :\n",
    "- **NUTRIENT_BOUNDS** : Plages acceptables pour chaque nutriment\n",
    "- **ANOMALY_THRESHOLDS** : Limites pour d√©tecter les valeurs aberrantes\n",
    "- **MIN_NUTRIENTS_REQUIRED** : Minimum de nutriments pour inclure un produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plages de validit√© pour chaque nutriment (min, max) en g/100g\n",
    "NUTRIENT_BOUNDS = {\n",
    "    \"energy_kcal_100g\": (0, 900),\n",
    "    \"fat_100g\": (0, 100),\n",
    "    \"saturated_fat_100g\": (0, 100),\n",
    "    \"sugars_100g\": (0, 100),\n",
    "    \"salt_100g\": (0, 100),\n",
    "    \"proteins_100g\": (0, 100),\n",
    "    \"fiber_100g\": (0, 100),\n",
    "    \"sodium_100g\": (0, 40),\n",
    "}\n",
    "\n",
    "# Seuils pour d√©tecter les anomalies extr√™mes\n",
    "ANOMALY_THRESHOLDS = {\n",
    "    \"sugars_100g\": 80,       # Pas de produit avec > 80g de sucre/100g\n",
    "    \"salt_100g\": 25,         # Pas de produit avec > 25g de sel/100g\n",
    "    \"proteins_100g\": 90,     # Pas de produit avec > 90g prot√©ines/100g\n",
    "}\n",
    "\n",
    "# Facteur de conversion sel ‚Üî sodium (formule chimique NaCl)\n",
    "SALT_SODIUM_FACTOR = 2.5\n",
    "\n",
    "# Minimum de nutriments pour garder un produit\n",
    "MIN_NUTRIENTS_REQUIRED = 3\n",
    "\n",
    "print(f\"‚úì Constantes d√©finies\")\n",
    "print(f\"  ‚Üí {len(NUTRIENT_BOUNDS)} nutriments √† valider\")\n",
    "print(f\"  ‚Üí {len(ANOMALY_THRESHOLDS)} seuils d'anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Fonction utilitaire de nettoyage\n",
    "\n",
    "Convertit les colonnes texte en valeurs num√©riques valides :\n",
    "1. Remplace virgules par points (notation europ√©enne)\n",
    "2. Rejette les URLs\n",
    "3. Valide le format num√©rique (y compris notation scientifique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric(col_name):\n",
    "    \"\"\"Convertit une colonne texte en nombre valide (ou NULL si invalide).\"\"\"\n",
    "    col = F.col(col_name)\n",
    "    # Normalise la d√©cimale (virgule ‚Üí point)\n",
    "    col = F.regexp_replace(col, \",\", \".\")\n",
    "    # Rejette les URLs\n",
    "    col = F.when(col.like(\"http%\"), None).otherwise(col)\n",
    "    # Valide le format num√©rique (accepte aussi notation sci : 1.2e-3)\n",
    "    col = F.when(\n",
    "        col.rlike(r'^[+-]?[0-9]+(\\.[0-9]+)?([eE][+-]?[0-9]+)?$'),\n",
    "        col.cast(\"double\")\n",
    "    ).otherwise(None)\n",
    "    return col\n",
    "\n",
    "print(\"‚úì Fonction clean_numeric d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Cr√©ation de la SparkSession\n",
    "\n",
    "Lance le contexte Spark avec configuration pour parser les dates au format legacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OFF_ETL\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì SparkSession cr√©√©e : {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Lecture Bronze (CSV)\n",
    "\n",
    "Charge le fichier CSV 300k produits avec :\n",
    "- S√©parateur TAB\n",
    "- Cast explicite de chaque colonne\n",
    "- Nettoyage des colonnes num√©riques via `clean_numeric()`\n",
    "- Limitation √† 500k lignes pour test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture CSV brute\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \"\\t\")          # S√©parateur TAB\n",
    "    .option(\"inferSchema\", \"false\") # Pas d'inf√©rence, cast explicite\n",
    "    .load(\"data/300k_off.csv\")\n",
    ")\n",
    "\n",
    "# S√©lection et cast des colonnes utiles\n",
    "df_raw = df_raw.select(\n",
    "    # Identifiants et dates\n",
    "    F.col(\"code\").cast(\"string\"),\n",
    "    F.col(\"creator\").cast(\"string\"),\n",
    "    F.col(\"created_datetime\").cast(\"string\"),\n",
    "    F.col(\"last_modified_t\").cast(\"int\"),\n",
    "    F.col(\"last_modified_datetime\").cast(\"string\"),\n",
    "    F.col(\"last_modified_by\").cast(\"string\"),\n",
    "    F.col(\"last_updated_t\").cast(\"int\"),\n",
    "    F.col(\"last_updated_datetime\").cast(\"string\"),\n",
    "    \n",
    "    # Descriptifs\n",
    "    F.col(\"product_name\").cast(\"string\"),\n",
    "    F.col(\"generic_name\").cast(\"string\"),\n",
    "    F.col(\"quantity\").cast(\"string\"),\n",
    "    F.col(\"packaging\").cast(\"string\"),\n",
    "    F.col(\"packaging_tags\").cast(\"string\"),\n",
    "    F.col(\"packaging_en\").cast(\"string\"),\n",
    "    F.col(\"packaging_text\").cast(\"string\"),\n",
    "    \n",
    "    # Cat√©gorisation\n",
    "    F.col(\"brands\").cast(\"string\"),\n",
    "    F.col(\"categories\").cast(\"string\"),\n",
    "    F.col(\"countries\").cast(\"string\"),\n",
    "    \n",
    "    # Scores nutritionnels\n",
    "    clean_numeric(\"nutriscore_score\").alias(\"nutriscore_score\"),\n",
    "    F.col(\"nutriscore_grade\").cast(\"string\").alias(\"nutriscore_grade\"),\n",
    "    clean_numeric(\"nova_group\").cast(\"int\").alias(\"nova_group\"),\n",
    "    \n",
    "    # Classification PNNS\n",
    "    F.col(\"pnns_groups_1\").cast(\"string\"),\n",
    "    F.col(\"pnns_groups_2\").cast(\"string\"),\n",
    "    \n",
    "    # Nutriments (conversion et nettoyage)\n",
    "    clean_numeric(\"energy-kcal_100g\").alias(\"energy_kcal_100g\"),\n",
    "    clean_numeric(\"fat_100g\").alias(\"fat_100g\"),\n",
    "    clean_numeric(\"saturated-fat_100g\").alias(\"saturated_fat_100g\"),\n",
    "    clean_numeric(\"sugars_100g\").alias(\"sugars_100g\"),\n",
    "    clean_numeric(\"salt_100g\").alias(\"salt_100g\"),\n",
    "    clean_numeric(\"proteins_100g\").alias(\"proteins_100g\"),\n",
    "    clean_numeric(\"fiber_100g\").alias(\"fiber_100g\"),\n",
    "    clean_numeric(\"sodium_100g\").alias(\"sodium_100g\"),\n",
    ")\n",
    "\n",
    "# Limite pour test (enlever en prod)\n",
    "df_raw = df_raw.limit(500000)\n",
    "\n",
    "print(f\"‚úì Bronze charg√©e : {df_raw.count()} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : D√©duplication Silver (SCD2 Ready)\n",
    "\n",
    "Pour chaque code produit, garde la version la plus r√©cente via `last_modified_t`.\n",
    "Utilise une fen√™tre Spark avec `row_number()` pour identifier et filtrer les doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre pr√©alable : √©carte produits sans code ou sans nom\n",
    "df_filtered = (\n",
    "    df_raw\n",
    "    .filter(F.col(\"code\").isNotNull())\n",
    "    .filter(F.col(\"product_name\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"Apr√®s filtre de base : {df_filtered.count()} lignes\")\n",
    "\n",
    "# D√©duplication : garde la version la plus r√©cente par code\n",
    "w = Window.partitionBy(\"code\").orderBy(F.col(\"last_modified_t\").desc_nulls_last())\n",
    "\n",
    "df_silver = (\n",
    "    df_filtered\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)  # Garde seulement la premi√®re (la plus r√©cente)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì Silver d√©dupliqu√©e : {df_silver.count()} lignes uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 : Validation des nutriments (NUTRIENT_BOUNDS)\n",
    "\n",
    "Met les valeurs hors bornes √† NULL selon NUTRIENT_BOUNDS.\n",
    "Exemple : √©nergies > 900 kcal/100g ‚Üí consid√©r√©es comme erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applique les bornes de validation √† chaque nutriment\n",
    "for col_name, (min_val, max_val) in NUTRIENT_BOUNDS.items():\n",
    "    if col_name in df_silver.columns:\n",
    "        df_silver = df_silver.withColumn(\n",
    "            col_name,\n",
    "            F.when(\n",
    "                (F.col(col_name) >= min_val) & (F.col(col_name) <= max_val),\n",
    "                F.col(col_name),\n",
    "            ).otherwise(None),  # Hors bornes ‚Üí NULL\n",
    "        )\n",
    "\n",
    "print(f\"‚úì Nutriments valid√©s contre NUTRIENT_BOUNDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7 : Coh√©rence gras satur√© ‚â§ gras total\n",
    "\n",
    "V√©rifie que `saturated_fat ‚â§ fat_total`. Si violation ‚Üí met saturated_fat √† NULL.\n",
    "C'est une r√®gle chimique obligatoire (les graisses satur√©es sont un sous-ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostique : compte les incoh√©rences avant correction\n",
    "nb_incoherent_satfat = df_silver.filter(\n",
    "    (F.col(\"saturated_fat_100g\").isNotNull()) &\n",
    "    (F.col(\"fat_100g\").isNotNull()) &\n",
    "    (F.col(\"saturated_fat_100g\") > F.col(\"fat_100g\"))\n",
    ").count()\n",
    "\n",
    "print(f\"Coh√©rence : {nb_incoherent_satfat} produits avec saturated_fat > fat\")\n",
    "\n",
    "# Correction : met √† NULL les gras satur√©s incoh√©rents\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"saturated_fat_100g\",\n",
    "    F.when(\n",
    "        F.col(\"saturated_fat_100g\") > F.col(\"fat_100g\"),\n",
    "        None  # Violation chimique ‚Üí NULL\n",
    "    ).otherwise(F.col(\"saturated_fat_100g\")),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Gras satur√© corrig√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8 : Harmonisation sel / sodium\n",
    "\n",
    "Compl√®te les valeurs manquantes via la relation : **sodium = sel / 2.5**\n",
    "\n",
    "Si sodium absent ‚Üí calcule depuis sel\n",
    "Si sel absent ‚Üí calcule depuis sodium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si sodium manquant, le calcule depuis sel\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"sodium_100g\",\n",
    "    F.when(\n",
    "        F.col(\"sodium_100g\").isNull() & F.col(\"salt_100g\").isNotNull(),\n",
    "        F.round(F.col(\"salt_100g\") / SALT_SODIUM_FACTOR, 2),\n",
    "    ).otherwise(F.col(\"sodium_100g\")),\n",
    ")\n",
    "\n",
    "# Si sel manquant, le calcule depuis sodium\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"salt_100g\",\n",
    "    F.when(\n",
    "        F.col(\"salt_100g\").isNull() & F.col(\"sodium_100g\").isNotNull(),\n",
    "        F.round(F.col(\"sodium_100g\") * SALT_SODIUM_FACTOR, 2),\n",
    "    ).otherwise(F.col(\"salt_100g\")),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Sel/sodium harmonis√©s (facteur: {SALT_SODIUM_FACTOR})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9 : Compte de nutriments pr√©sents\n",
    "\n",
    "Pour chaque produit, compte le nombre de nutriments non-NULL.\n",
    "Filtre ensuite : garde seulement les produits avec ‚â• MIN_NUTRIENTS_REQUIRED nutriments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes nutriments √† compter\n",
    "nutrient_cols = [\n",
    "    \"energy_kcal_100g\",\n",
    "    \"fat_100g\",\n",
    "    \"saturated_fat_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"salt_100g\",\n",
    "    \"proteins_100g\",\n",
    "    \"fiber_100g\",\n",
    "]\n",
    "\n",
    "# Ajoute colonne de compte\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"nutrients_present\",\n",
    "    sum([F.when(F.col(c).isNotNull(), 1).otherwise(0) for c in nutrient_cols]),\n",
    ")\n",
    "\n",
    "nb_before = df_silver.count()\n",
    "\n",
    "# Filtre : garde seulement les produits avec assez de nutriments\n",
    "df_silver = df_silver.filter(F.col(\"nutrients_present\") >= MIN_NUTRIENTS_REQUIRED)\n",
    "\n",
    "nb_after = df_silver.count()\n",
    "nb_removed_nutrients = nb_before - nb_after\n",
    "\n",
    "print(f\"‚úì Filtre nutriments : {nb_removed_nutrients} produits supprim√©s\")\n",
    "print(f\"  ‚Üí {nb_after} produits conserv√©s (‚â• {MIN_NUTRIENTS_REQUIRED} nutriments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10 : Suppression des anomalies extr√™mes\n",
    "\n",
    "Utilise ANOMALY_THRESHOLDS pour rejeter les valeurs aberrantes.\n",
    "Exemple : aucun produit normal ne contient 80g+ de sucre/100g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_removed = {}  # Suivi des suppressions par colonne\n",
    "\n",
    "for col_name, threshold in ANOMALY_THRESHOLDS.items():\n",
    "    if col_name in df_silver.columns:\n",
    "        before_col = df_silver.count()\n",
    "        # Filtre : garde les NULLs ou les valeurs <= seuil\n",
    "        df_silver = df_silver.filter(\n",
    "            F.col(col_name).isNull() | (F.col(col_name) <= threshold)\n",
    "        )\n",
    "        after_col = df_silver.count()\n",
    "        anomalies_removed[col_name] = before_col - after_col\n",
    "\n",
    "print(f\"‚úì Anomalies supprim√©es :\")\n",
    "for col_name, count in anomalies_removed.items():\n",
    "    print(f\"  ‚Üí {col_name} : {count} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11 : Validation somme nutriments ‚â§ 110g\n",
    "\n",
    "V√©rifie que la somme gras + sucres + prot√©ines + fibres + sel ‚â§ 110g/100g\n",
    "(car 100g max, mais eau + autres composants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule la somme des nutriments cl√©s\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"_nutrient_sum\",\n",
    "    F.coalesce(F.col(\"fat_100g\"), F.lit(0))\n",
    "    + F.coalesce(F.col(\"sugars_100g\"), F.lit(0))\n",
    "    + F.coalesce(F.col(\"proteins_100g\"), F.lit(0))\n",
    "    + F.coalesce(F.col(\"fiber_100g\"), F.lit(0))\n",
    "    + F.coalesce(F.col(\"salt_100g\"), F.lit(0)),\n",
    ")\n",
    "\n",
    "nb_before_sum = df_silver.count()\n",
    "df_silver = df_silver.filter(F.col(\"_nutrient_sum\") <= 110)\n",
    "nb_removed_sum = nb_before_sum - df_silver.count()\n",
    "\n",
    "print(f\"‚úì Validation somme nutriments : {nb_removed_sum} produits supprim√©s\")\n",
    "print(f\"  ‚Üí {df_silver.count()} produits avec somme coh√©rente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12 : Normalisation des scores\n",
    "\n",
    "Valide et normalise :\n",
    "- Nutri-Score ‚Üí uniquement A/B/C/D/E en majuscules\n",
    "- NOVA Group ‚Üí uniquement 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutri-Score : garde seulement les grades valides (A-E en majuscules)\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"nutriscore_grade\",\n",
    "    F.when(\n",
    "        F.upper(F.col(\"nutriscore_grade\")).isin([\"A\", \"B\", \"C\", \"D\", \"E\"]),\n",
    "        F.upper(F.col(\"nutriscore_grade\")),\n",
    "    ).otherwise(None),\n",
    ")\n",
    "\n",
    "# NOVA Group : garde seulement 1, 2, 3, 4\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"nova_group\",\n",
    "    F.when(F.col(\"nova_group\").between(1, 4), F.col(\"nova_group\")).otherwise(None),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Scores normalis√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13 : Extraction valeurs principales\n",
    "\n",
    "De champs multi-valu√©s (listes s√©par√©es par virgule), extrait la premi√®re valeur utile :\n",
    "- Premi√®re marque dans `brands`\n",
    "- Premier pays dans `countries` (nettoie le pr√©fixe \"en:\")\n",
    "- Premi√®re cat√©gorie dans `categories` (nettoie le pr√©fixe \"en:\")\n",
    "\n",
    "Puis filtre les valeurs \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrait la premi√®re valeur de chaque liste\n",
    "df_silver = (\n",
    "    df_silver\n",
    "    .withColumn(\"brand_primary\", F.trim(F.split(F.col(\"brands\"), \",\").getItem(0)))\n",
    "    .withColumn(\n",
    "        \"country_primary\",\n",
    "        F.trim(F.regexp_replace(F.split(F.col(\"countries\"), \",\").getItem(0), \"^en:\", \"\")),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"category_primary\",\n",
    "        F.trim(F.regexp_replace(F.split(F.col(\"categories\"), \",\").getItem(0), \"^en:\", \"\")),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Nettoie les valeurs \"unknown\"\n",
    "cols_to_clean_unknown = [\n",
    "    \"brand_primary\",\n",
    "    \"country_primary\",\n",
    "    \"category_primary\",\n",
    "    \"pnns_groups_1\",\n",
    "    \"pnns_groups_2\",\n",
    "]\n",
    "\n",
    "for col_name in cols_to_clean_unknown:\n",
    "    if col_name in df_silver.columns:\n",
    "        df_silver = df_silver.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.lower(F.col(col_name)) == \"unknown\", None).otherwise(F.col(col_name)),\n",
    "        )\n",
    "\n",
    "print(f\"‚úì Valeurs principales extraites et nettoy√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14 : Score de compl√©tude\n",
    "\n",
    "Mesure de 0 √† 1 bas√©e sur :\n",
    "- Pr√©sence du nom du produit (25%)\n",
    "- Pr√©sence des marques (25%)\n",
    "- Pr√©sence des cat√©gories (25%)\n",
    "- Ratio nutriments pr√©sents / nutriments totaux (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"completeness_score\",\n",
    "    (\n",
    "        F.when(F.col(\"product_name\").isNotNull(), 1).otherwise(0) +\n",
    "        F.when(F.col(\"brands\").isNotNull(), 1).otherwise(0) +\n",
    "        F.when(F.col(\"categories\").isNotNull(), 1).otherwise(0) +\n",
    "        (F.col(\"nutrients_present\") / F.lit(len(nutrient_cols)))\n",
    "    ) / F.lit(4.0)\n",
    ")\n",
    "\n",
    "avg_completeness = df_silver.agg(F.avg(\"completeness_score\")).first()[0] or 0.0\n",
    "print(f\"‚úì Score compl√©tude calcul√© (moyenne: {avg_completeness:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15 : Probl√®mes de qualit√© (JSON)\n",
    "\n",
    "Cr√©e un array JSON listant les probl√®mes de chaque produit :\n",
    "- missing_product_name\n",
    "- missing_brands\n",
    "- missing_categories\n",
    "- no_nutrients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver = df_silver.withColumn(\n",
    "    \"quality_issues\",\n",
    "    F.array(\n",
    "        F.when(F.col(\"product_name\").isNull(), F.lit(\"missing_product_name\")),\n",
    "        F.when(F.col(\"brands\").isNull(), F.lit(\"missing_brands\")),\n",
    "        F.when(F.col(\"categories\").isNull(), F.lit(\"missing_categories\")),\n",
    "        F.when(F.col(\"nutrients_present\") == 0, F.lit(\"no_nutrients\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filtre pour ne garder que les non-NULLs\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"quality_issues\",\n",
    "    F.expr(\"filter(quality_issues, x -> x is not null)\")\n",
    ")\n",
    "\n",
    "# S√©rialise en JSON pour stockage\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"quality_issues_json\",\n",
    "    F.to_json(F.col(\"quality_issues\"))\n",
    ").drop(\"quality_issues\")\n",
    "\n",
    "print(f\"‚úì Probl√®mes de qualit√© identifi√©s et s√©rialis√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16 : Diagnostic et statistiques Silver\n",
    "\n",
    "Affiche :\n",
    "- Dimensions du DataFrame\n",
    "- Sch√©ma complet\n",
    "- Statistiques descriptives (colonnes num√©riques)\n",
    "- Taux de remplissage par colonne\n",
    "- Aper√ßu des donn√©es\n",
    "- Distribution Nutri-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIC SILVER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "nb_rows = df_silver.count()\n",
    "nb_cols = len(df_silver.columns)\n",
    "print(f\"\\nüìä Dimensions : {nb_rows} lignes √ó {nb_cols} colonnes\")\n",
    "\n",
    "print(\"\\nüîç Sch√©ma du DataFrame :\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Colonnes num√©riques pour statistiques\n",
    "numeric_cols = [\n",
    "    f.name for f in df_silver.schema.fields\n",
    "    if f.dataType.simpleString() in (\"double\", \"int\", \"bigint\")\n",
    "]\n",
    "\n",
    "if numeric_cols:\n",
    "    print(\"\\nüìà Statistiques descriptives (colonnes num√©riques) :\")\n",
    "    df_silver.select(*numeric_cols).describe().show(truncate=20)\n",
    "\n",
    "print(\"\\nüíß Taux de remplissage par colonne :\")\n",
    "print(f\"{'Colonne':<30} {'Non-Null':>10} {'Null':>10} {'Remplissage':>12}\")\n",
    "print(\"-\" * 62)\n",
    "for c in df_silver.columns:\n",
    "    non_null = df_silver.filter(F.col(c).isNotNull()).count()\n",
    "    null_count = nb_rows - non_null\n",
    "    pct = (non_null / nb_rows * 100) if nb_rows > 0 else 0\n",
    "    print(f\"{c:<30} {non_null:>10} {null_count:>10} {pct:>11.2f}%\")\n",
    "\n",
    "print(\"\\nüëÄ Aper√ßu (5 premi√®res lignes) :\")\n",
    "df_silver.show(5, truncate=50)\n",
    "\n",
    "print(f\"\\nüíæ Estimation m√©moire : ~{nb_rows * nb_cols * 50 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 17 : Distribution Nutri-Score et compl√©tude\n",
    "\n",
    "Affiche la distribution des notes (A/B/C/D/E) et le score de compl√©tude moyen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä DISTRIBUTION NUTRI-SCORE\")\n",
    "print(\"-\" * 40)\n",
    "df_silver.groupBy(\"nutriscore_grade\").count().orderBy(\"nutriscore_grade\").show()\n",
    "\n",
    "avg_comp = df_silver.agg(F.avg(\"completeness_score\")).first()[0] or 0.0\n",
    "print(f\"\\nüìç Score de compl√©tude moyen : {avg_comp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 18 : Cr√©ation des dimensions (Fact & Dimension Tables)\n",
    "\n",
    "### dim_brand\n",
    "Extrait les marques uniques et les regroupe avec une cl√© de remplacement (`brand_sk`).\n",
    "Utilise `monotonically_increasing_id()` pour g√©n√©rer des SKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIM_BRAND : liste unique des marques\n",
    "df_dim_brand = (\n",
    "    df_silver\n",
    "    .select(F.explode(F.split(F.col(\"brands\"), \",\")).alias(\"brand_name_raw\"))\n",
    "    .withColumn(\"brand_name\", F.trim(F.col(\"brand_name_raw\")))\n",
    "    .filter(F.col(\"brand_name\").isNotNull() & (F.col(\"brand_name\") != \"\"))\n",
    "    .dropDuplicates([\"brand_name\"])\n",
    "    .withColumn(\"brand_sk\", F.monotonically_increasing_id())\n",
    "    .select(\"brand_sk\", \"brand_name\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì dim_brand cr√©√©e : {df_dim_brand.count()} marques uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dim_country\n",
    "Extrait les pays uniques, nettoie le pr√©fixe \"en:\", regroupe avec cl√© de remplacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIM_COUNTRY : liste unique des pays\n",
    "df_dim_country = (\n",
    "    df_silver\n",
    "    .select(F.explode(F.split(F.col(\"countries\"), \",\")).alias(\"country_name_raw\"))\n",
    "    .withColumn(\n",
    "        \"country_name\",\n",
    "        F.trim(F.regexp_replace(F.col(\"country_name_raw\"), \"^en:\", \"\")),\n",
    "    )\n",
    "    .filter(F.col(\"country_name\").isNotNull() & (F.col(\"country_name\") != \"\"))\n",
    "    .dropDuplicates([\"country_name\"])\n",
    "    .withColumn(\"country_sk\", F.monotonically_increasing_id())\n",
    "    .select(\"country_sk\", \"country_name\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì dim_country cr√©√©e : {df_dim_country.count()} pays uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dim_category\n",
    "Extrait les cat√©gories uniques, nettoie le pr√©fixe \"en:\", regroupe avec cl√© de remplacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIM_CATEGORY : liste unique des cat√©gories\n",
    "df_dim_category = (\n",
    "    df_silver\n",
    "    .select(F.explode(F.split(F.col(\"categories\"), \",\")).alias(\"category_name_raw\"))\n",
    "    .withColumn(\n",
    "        \"category_name\",\n",
    "        F.trim(F.regexp_replace(F.col(\"category_name_raw\"), \"^en:\", \"\")),\n",
    "    )\n",
    "    .filter(F.col(\"category_name\").isNotNull() & (F.col(\"category_name\") != \"\"))\n",
    "    .dropDuplicates([\"category_name\"])\n",
    "    .withColumn(\"category_sk\", F.monotonically_increasing_id())\n",
    "    .select(\"category_sk\", \"category_name\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì dim_category cr√©√©e : {df_dim_category.count()} cat√©gories uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 19 : dim_product avec SCD2 (Slowly Changing Dimension Type 2)\n",
    "\n",
    "Cr√©e la dimension produit avec support SCD2 :\n",
    "1. **Enrichissement** : join avec dim_brand, dim_country, dim_category\n",
    "2. **Hash des attributs** : SHA256 de tous les champs pour d√©tecter changements\n",
    "3. **S√©lection finale** : colonnes n√©cessaires pour SCD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichit silver avec les noms de dimensions pour les joins\n",
    "df_products_enriched = (\n",
    "    df_silver\n",
    "    .withColumn(\"brand_name\", F.col(\"brand_primary\"))\n",
    "    .withColumn(\"country_name\", F.col(\"country_primary\"))\n",
    "    .withColumn(\"category_name\", F.col(\"category_primary\"))\n",
    ")\n",
    "\n",
    "# Dimension produit de base (avec doublons si m√™me produit plusieurs fois)\n",
    "df_dim_product = (\n",
    "    df_products_enriched\n",
    "    .select(\n",
    "        \"code\",\n",
    "        \"product_name\",\n",
    "        \"brand_name\",\n",
    "        \"country_name\",\n",
    "        \"category_name\",\n",
    "        \"nutriscore_grade\",\n",
    "        \"nova_group\",\n",
    "        \"pnns_groups_1\",\n",
    "        \"pnns_groups_2\",\n",
    "        \"completeness_score\",\n",
    "        \"quality_issues_json\"\n",
    "    )\n",
    "    .dropDuplicates([\"code\"])  # Garde 1 version par code pour ce run\n",
    ")\n",
    "\n",
    "# Join avec les dimensions pour r√©cup√©rer les SKs\n",
    "df_dim_product = (\n",
    "    df_dim_product\n",
    "    .join(df_dim_brand, on=\"brand_name\", how=\"left\")\n",
    "    .join(df_dim_country, on=\"country_name\", how=\"left\")\n",
    "    .join(df_dim_category, on=\"category_name\", how=\"left\")\n",
    "    .withColumn(\"product_sk\", F.monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# Hash SHA256 pour d√©tecter les changements (SCD2)\n",
    "cols_for_hash = [\n",
    "    \"product_name\",\n",
    "    \"brand_sk\",\n",
    "    \"country_sk\",\n",
    "    \"category_sk\",\n",
    "    \"nutriscore_grade\",\n",
    "    \"nova_group\",\n",
    "    \"pnns_groups_1\",\n",
    "    \"pnns_groups_2\",\n",
    "    \"completeness_score\",\n",
    "    \"quality_issues_json\",\n",
    "]\n",
    "\n",
    "df_dim_product = df_dim_product.withColumn(\n",
    "    \"attr_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\".\n",
    "           *[F.col(c).cast(\"string\") for c in cols_for_hash]), 256)\n",
    ")\n",
    "\n",
    "# S√©lection finale pour SCD2\n",
    "df_dim_product_new = df_dim_product.select(\n",
    "    \"code\",\n",
    "    \"product_name\",\n",
    "    \"brand_sk\",\n",
    "    \"country_sk\",\n",
    "    \"category_sk\",\n",
    "    \"nutriscore_grade\",\n",
    "    \"nova_group\",\n",
    "    \"pnns_groups_1\",\n",
    "    \"pnns_groups_2\",\n",
    "    \"completeness_score\",\n",
    "    \"quality_issues_json\",\n",
    "    \"attr_hash\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì dim_product_new cr√©√©e : {df_dim_product_new.count()} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 20 : dim_time enrichie\n",
    "\n",
    "Cr√©e la dimension temps avec :\n",
    "- Date\n",
    "- Ann√©e, mois, jour\n",
    "- Semaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrait dates uniques et ajoute composantes temporelles\n",
    "df_time = (\n",
    "    df_silver\n",
    "    .select(\"last_modified_datetime\")\n",
    "    .withColumn(\"date\", F.to_date(\"last_modified_datetime\"))\n",
    "    .dropDuplicates([\"date\"])\n",
    "    .withColumn(\"time_sk\", F.monotonically_increasing_id())\n",
    "    .select(\"time_sk\", \"date\")\n",
    ")\n",
    "\n",
    "# Ajoute colonnes temporelles\n",
    "df_dim_time_pg = df_time.select(\n",
    "    \"time_sk\",\n",
    "    F.col(\"date\").alias(\"date\"),\n",
    "    F.year(\"date\").alias(\"year\"),\n",
    "    F.month(\"date\").alias(\"month\"),\n",
    "    F.dayofmonth(\"date\").alias(\"day\"),\n",
    "    F.weekofyear(\"date\").alias(\"week\")\n",
    ")\n",
    "\n",
    "print(f\"‚úì dim_time cr√©√©e : {df_dim_time_pg.count()} dates uniques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 21 : Configuration JDBC PostgreSQL\n",
    "\n",
    "Param√®tres de connexion √† la base de donn√©es PostgreSQL.\n",
    "√Ä adapter avec vos identifiants r√©els."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres JDBC pour PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/Tp_OFF\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"batchsize\": \"5000\"\n",
    "}\n",
    "\n",
    "print(f\"‚úì Configuration JDBC d√©finie\")\n",
    "print(f\"  ‚Üí URL: {jdbc_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 22 : √âcriture des dimensions simples\n",
    "\n",
    "√âcrit les dimensions qui ne changent pas (non-SCD2) en mode APPEND :\n",
    "- dim_brand\n",
    "- dim_country\n",
    "- dim_category\n",
    "- dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âcriture des dimensions en mode APPEND (accumulation)\n",
    "print(\"\\nüìù √âcriture des dimensions...\")\n",
    "\n",
    "df_dim_brand.coalesce(4).write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(jdbc_url, \"off_dm.dim_brand\", properties=jdbc_properties)\n",
    "print(\"  ‚úì dim_brand √©crite\")\n",
    "\n",
    "df_dim_country.coalesce(4).write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(jdbc_url, \"off_dm.dim_country\", properties=jdbc_properties)\n",
    "print(\"  ‚úì dim_country √©crite\")\n",
    "\n",
    "df_dim_category.coalesce(4).write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(jdbc_url, \"off_dm.dim_category\", properties=jdbc_properties)\n",
    "print(\"  ‚úì dim_category √©crite\")\n",
    "\n",
    "df_dim_time_pg.coalesce(4).write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(jdbc_url, \"off_dm.dim_time\", properties=jdbc_properties)\n",
    "print(\"  ‚úì dim_time √©crite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 23 : Gestion SCD2 pour dim_product\n",
    "\n",
    "Impl√©mente le \"Slowly Changing Dimension Type 2\" pour la dimension produit :\n",
    "\n",
    "1. **Lire** la dimension existante (ou cr√©er vide au 1er run)\n",
    "2. **Identifier** :\n",
    "   - Nouveaux produits (code n'existe pas)\n",
    "   - Produits inchang√©s (hash identique)\n",
    "   - Produits modifi√©s (hash diff√©rent)\n",
    "3. **Fermer** les anciennes lignes (effective_to = now, is_current = False)\n",
    "4. **Ins√©rer** les nouvelles versions\n",
    "\n",
    "Cela garde l'historique complet des changements produit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è Gestion SCD2 pour dim_product...\")\n",
    "\n",
    "current_ts = F.current_timestamp()\n",
    "\n",
    "# Lecture de la dimension existante\n",
    "try:\n",
    "    df_dim_product_existing = spark.read.jdbc(\n",
    "        jdbc_url,\n",
    "        \"off_dm.dim_product\",\n",
    "        properties=jdbc_properties\n",
    "    )\n",
    "    print(\"  ‚úì dim_product existante lue\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ÑπÔ∏è Premi√®re ex√©cution (dim_product cr√©√©e √† partir de z√©ro)\")\n",
    "    # Sch√©ma pour premi√®re cr√©ation\n",
    "    df_dim_product_existing = spark.createDataFrame(\n",
    "        [],\n",
    "        schema=df_dim_product_new.schema\n",
    "        .add(\"product_sk\", \"long\")\n",
    "        .add(\"effective_from\", \"timestamp\")\n",
    "        .add(\"effective_to\", \"timestamp\")\n",
    "        .add(\"is_current\", \"boolean\")\n",
    "    )\n",
    "\n",
    "# Lignes courantes existantes\n",
    "df_dim_product_curr = df_dim_product_existing.filter(F.col(\"is_current\") == True)\n",
    "\n",
    "# Join nouveau vs existant\n",
    "joined = df_dim_product_new.alias(\"n\").join(\n",
    "    df_dim_product_curr.select(\n",
    "        \"product_sk\", \"code\", \"attr_hash\"\n",
    "    ).alias(\"e\"),\n",
    "    on=\"code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Cat√©gorise les produits\n",
    "# Nouveaux produits (n'existaient pas)\n",
    "df_new_products = joined.filter(F.col(\"e.code\").isNull()).select(\"n.*\")\n",
    "print(f\"  ‚Üí {df_new_products.count()} nouveaux produits\")\n",
    "\n",
    "# Produits inchang√©s (hash identique)\n",
    "df_unchanged = joined.filter(\n",
    "    (F.col(\"e.code\").isNotNull()) &\n",
    "    (F.col(\"n.attr_hash\") == F.col(\"e.attr_hash\"))\n",
    ")\n",
    "print(f\"  ‚Üí {df_unchanged.count()} produits inchang√©s\")\n",
    "\n",
    "# Produits modifi√©s (hash diff√©rent)\n",
    "df_changed = joined.filter(\n",
    "    (F.col(\"e.code\").isNotNull()) &\n",
    "    (F.col(\"n.attr_hash\") != F.col(\"e.attr_hash\"))\n",
    ").select(\"n.*\", \"e.product_sk\")\n",
    "print(f\"  ‚Üí {df_changed.count()} produits modifi√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 24 : Fermeture des anciennes lignes SCD2\n",
    "\n",
    "Pour les produits modifi√©s, met √† jour les anciennes lignes :\n",
    "- `effective_to` = timestamp actuel\n",
    "- `is_current` = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ferme les anciennes lignes des produits modifi√©s\n",
    "df_dim_product_to_close = df_dim_product_existing.join(\n",
    "    df_changed.select(\"product_sk\").distinct(),\n",
    "    on=\"product_sk\",\n",
    "    how=\"inner\"\n",
    ").withColumn(\"effective_to\", current_ts) \\\n",
    " .withColumn(\"is_current\", F.lit(False))\n",
    "\n",
    "print(f\"  ‚Üí {df_dim_product_to_close.count()} anciennes lignes √† fermer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 25 : Insertion des nouvelles lignes SCD2\n",
    "\n",
    "Pr√©pare les nouvelles lignes √† ins√©rer :\n",
    "- **Nouveaux produits** : directement ajout√©s\n",
    "- **Produits modifi√©s** : cr√©ent une nouvelle ligne (nouvelle version)\n",
    "\n",
    "Ajoute les colonnes SCD2 :\n",
    "- `effective_from` = timestamp actuel\n",
    "- `effective_to` = NULL (ligne courante)\n",
    "- `is_current` = TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelles lignes pour les nouveaux produits\n",
    "df_new_for_insert = df_new_products.withColumn(\n",
    "    \"product_sk\",\n",
    "    F.monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# Nouvelles lignes pour les versions modifi√©es\n",
    "df_changed_for_insert = df_changed.drop(\"product_sk\").withColumn(\n",
    "    \"product_sk\",\n",
    "    F.monotonically_increasing_id()\n",
    ")\n",
    "\n",
    "# Combine nouveaux + modifi√©s\n",
    "df_dim_product_insert = df_new_for_insert.unionByName(df_changed_for_insert)\n",
    "\n",
    "# S√©lectionne colonnes et ajoute m√©tadonn√©es SCD2\n",
    "df_dim_product_insert = df_dim_product_insert.select(\n",
    "    \"product_sk\",\n",
    "    \"code\",\n",
    "    \"product_name\",\n",
    "    \"brand_sk\",\n",
    "    \"country_sk\",\n",
    "    \"category_sk\",\n",
    "    \"nutriscore_grade\",\n",
    "    \"nova_group\",\n",
    "    \"pnns_groups_1\",\n",
    "    \"pnns_groups_2\",\n",
    "    \"completeness_score\",\n",
    "    \"quality_issues_json\",\n",
    "    \"attr_hash\"\n",
    ").withColumn(\"effective_from\", current_ts) \\\n",
    " .withColumn(\"effective_to\", F.lit(None).cast(\"timestamp\")) \\\n",
    " .withColumn(\"is_current\", F.lit(True))\n",
    "\n",
    "print(f\"  ‚Üí {df_dim_product_insert.count()} nouvelles lignes √† ins√©rer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 26 : √âcriture SCD2 de dim_product\n",
    "\n",
    "√âcrit les modifications SCD2 :\n",
    "1. Les anciennes lignes ferm√©es (effective_to renseign√©)\n",
    "2. Les nouvelles lignes (nouvelles versions + nouvelles entr√©es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìù √âcriture SCD2 de dim_product...\")\n",
    "\n",
    "# √âcrit les anciennes lignes ferm√©es\n",
    "if df_dim_product_to_close.count() > 0:\n",
    "    df_dim_product_to_close.coalesce(4).write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(jdbc_url, \"off_dm.dim_product\", properties=jdbc_properties)\n",
    "    print(f\"  ‚úì {df_dim_product_to_close.count()} anciennes lignes ferm√©es\")\n",
    "else:\n",
    "    print(f\"  ‚ÑπÔ∏è Aucune ancienne ligne √† fermer\")\n",
    "\n",
    "# √âcrit les nouvelles lignes\n",
    "if df_dim_product_insert.count() > 0:\n",
    "    df_dim_product_insert.coalesce(4).write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(jdbc_url, \"off_dm.dim_product\", properties=jdbc_properties)\n",
    "    print(f\"  ‚úì {df_dim_product_insert.count()} nouvelles lignes ins√©r√©es\")\n",
    "else:\n",
    "    print(f\"  ‚ÑπÔ∏è Aucune nouvelle ligne\")\n",
    "\n",
    "print(\"\\n  ‚ÑπÔ∏è Recharge de dim_product pour la fact...\")\n",
    "# Recharge pour la table de faits\n",
    "df_dim_product_after = spark.read.jdbc(\n",
    "    jdbc_url,\n",
    "    \"off_dm.dim_product\",\n",
    "    properties=jdbc_properties\n",
    ")\n",
    "\n",
    "# Extrait les product_sk courants (is_current = True)\n",
    "df_dim_product_current_for_fact = df_dim_product_after \\\n",
    "    .filter(F.col(\"is_current\") == True) \\\n",
    "    .select(\"product_sk\", \"code\")\n",
    "\n",
    "print(f\"  ‚úì {df_dim_product_current_for_fact.count()} product_sk courants pour la fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 27 : Cr√©ation de la table de faits\n",
    "\n",
    "Construite via joins avec les dimensions :\n",
    "1. S√©lectionne les mesures et dimensions du silver\n",
    "2. Join avec dim_time (date ‚Üí time_sk)\n",
    "3. Join avec dim_product (code ‚Üí product_sk)\n",
    "4. Attribue un identifiant unique (fact_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è Construction de la table de faits...\")\n",
    "\n",
    "df_fact = (\n",
    "    df_silver\n",
    "    .select(\n",
    "        \"code\",\n",
    "        \"last_modified_datetime\",\n",
    "        # Mesures nutriments\n",
    "        \"energy_kcal_100g\",\n",
    "        \"fat_100g\",\n",
    "        \"saturated_fat_100g\",\n",
    "        \"sugars_100g\",\n",
    "        \"salt_100g\",\n",
    "        \"proteins_100g\",\n",
    "        \"fiber_100g\",\n",
    "        \"sodium_100g\",\n",
    "        # Dimensions\n",
    "        \"nutriscore_grade\",\n",
    "        \"nova_group\",\n",
    "        \"completeness_score\",\n",
    "        \"quality_issues_json\"\n",
    "    )\n",
    "    # Ajoute time_sk via join\n",
    "    .withColumn(\"date\", F.to_date(\"last_modified_datetime\"))\n",
    "    .join(df_time, on=\"date\", how=\"left\")\n",
    "    # Ajoute product_sk via join\n",
    "    .join(df_dim_product_current_for_fact, on=\"code\", how=\"left\")\n",
    "    # Identifie chaque fait\n",
    "    .withColumn(\"fact_id\", F.monotonically_increasing_id())\n",
    "    # S√©lection finale\n",
    "    .select(\n",
    "        \"fact_id\",\n",
    "        \"product_sk\",\n",
    "        \"time_sk\",\n",
    "        \"energy_kcal_100g\",\n",
    "        \"fat_100g\",\n",
    "        \"saturated_fat_100g\",\n",
    "        \"sugars_100g\",\n",
    "        \"salt_100g\",\n",
    "        \"proteins_100g\",\n",
    "        \"fiber_100g\",\n",
    "        \"sodium_100g\",\n",
    "        \"nutriscore_grade\",\n",
    "        \"nova_group\",\n",
    "        \"completeness_score\",\n",
    "        \"quality_issues_json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"‚úì fact_nutrition_snapshot cr√©√©e : {df_fact.count()} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 28 : R√©sum√© du datamart\n",
    "\n",
    "Affiche les tailles de toutes les tables cr√©√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"R√âSUM√â DU DATAMART\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Dimensions\n",
    "  ‚Ä¢ dim_brand           : {df_dim_brand.count():>10,} lignes\n",
    "  ‚Ä¢ dim_country         : {df_dim_country.count():>10,} lignes\n",
    "  ‚Ä¢ dim_category        : {df_dim_category.count():>10,} lignes\n",
    "  ‚Ä¢ dim_product (SCD2)  : {df_dim_product_current_for_fact.count():>10,} versions courantes\n",
    "  ‚Ä¢ dim_time            : {df_dim_time_pg.count():>10,} dates\n",
    "\n",
    "üìà Faits\n",
    "  ‚Ä¢ fact_nutrition_snapshot : {df_fact.count():>10,} mesures\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 29 : √âcriture de la table de faits\n",
    "\n",
    "√âcrit fact_nutrition_snapshot en mode APPEND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìù √âcriture de la table de faits...\")\n",
    "\n",
    "df_fact_pg = df_fact.select(\n",
    "    \"fact_id\",\n",
    "    \"product_sk\",\n",
    "    \"time_sk\",\n",
    "    \"energy_kcal_100g\",\n",
    "    \"fat_100g\",\n",
    "    \"saturated_fat_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"salt_100g\",\n",
    "    \"proteins_100g\",\n",
    "    \"fiber_100g\",\n",
    "    \"sodium_100g\",\n",
    "    \"nutriscore_grade\",\n",
    "    \"nova_group\",\n",
    "    \"completeness_score\",\n",
    "    \"quality_issues_json\"\n",
    ")\n",
    "\n",
    "df_fact_pg.coalesce(4).write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(jdbc_url, \"off_dm.fact_nutrition_snapshot\", properties=jdbc_properties)\n",
    "\n",
    "print(f\"‚úì fact_nutrition_snapshot √©crite : {df_fact.count()} lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 30 : Collecte des m√©triques\n",
    "\n",
    "Cr√©e un dictionnaire JSON avec les KPIs du run :\n",
    "- Timestamps\n",
    "- Compteurs par table\n",
    "- Score de compl√©tude moyen\n",
    "- Distribution Nutri-Score\n",
    "- Compteurs d'anomalies corrig√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Calcul des m√©triques...\")\n",
    "\n",
    "# Distribution Nutri-Score\n",
    "nutri_dist_df = df_silver.groupBy(\"nutriscore_grade\").count()\n",
    "nutri_dist = {\n",
    "    (row[\"nutriscore_grade\"] if row[\"nutriscore_grade\"] is not None else \"NULL\"): int(row[\"count\"])\n",
    "    for row in nutri_dist_df.collect()\n",
    "}\n",
    "\n",
    "# Timestamp actuel\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "\n",
    "# M√©triques globales\n",
    "metrics = {\n",
    "    \"run_ts\": now_utc.isoformat(),\n",
    "    \"nb_raw_products\": df_raw.count(),\n",
    "    \"nb_silver_products\": df_silver.count(),\n",
    "    \"nb_dim_brand\": df_dim_brand.count(),\n",
    "    \"nb_dim_country\": df_dim_country.count(),\n",
    "    \"nb_dim_category\": df_dim_category.count(),\n",
    "    \"nb_dim_product_current\": df_dim_product_current_for_fact.count(),\n",
    "    \"nb_fact\": df_fact.count(),\n",
    "    \"avg_completeness_score\": float(\n",
    "        df_silver.agg(F.avg(\"completeness_score\")).first()[0] or 0.0\n",
    "    ),\n",
    "    \"nutriscore_distribution\": nutri_dist,\n",
    "    \"nb_removed_sugars_out_of_bounds\": anomalies_removed.get(\"sugars_100g\", 0),\n",
    "    \"nb_removed_salt_out_of_bounds\": anomalies_removed.get(\"salt_100g\", 0),\n",
    "    \"nb_removed_proteins_out_of_bounds\": anomalies_removed.get(\"proteins_100g\", 0),\n",
    "    \"nb_incoherent_saturated_fat_gt_fat\": nb_incoherent_satfat,\n",
    "}\n",
    "\n",
    "print(f\"‚úì M√©triques calcul√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 31 : Sauvegarde des m√©triques en JSON\n",
    "\n",
    "√âcrit les m√©triques dans un fichier `logs/metrics_YYYYMMDD_HHMMSS.json` pour suivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©e r√©pertoire logs s'il n'existe pas\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Sauvegarde des m√©triques\n",
    "metrics_file = f\"logs/metrics_{now_utc.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úì M√©triques sauvegard√©es : {metrics_file}\")\n",
    "print(f\"\\nüìã R√©sum√© des m√©triques :\")\n",
    "print(json.dumps(metrics, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
